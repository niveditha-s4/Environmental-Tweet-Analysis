{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513541b4-fb67-4b9f-823b-bd3b8be46af6",
   "metadata": {},
   "source": [
    "##  1) Vader sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8480b511-eb4c-4e77-a3df-ef4907247733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['year', 'cleaned_tweet'], dtype='object')\n",
      "   year                                      cleaned_tweet Sentiment\n",
      "0  2015  procreate coloring book 16 blooming chibi natu...  Positive\n",
      "1  2015  bear bones vehicles would out sell the planed ...  Negative\n",
      "2  2015  it sounds like youre picking up on the intense...  Positive\n",
      "3  2015  blends adventure crypto and wildlife conservat...  Positive\n",
      "4  2015  that alone raises questions about the nature o...  Negative\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"finaldataset.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Print column names to verify\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "\n",
    "# Initialize Vader Sentiment Analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "def get_sentiment_vader(text):\n",
    "    score = analyzer.polarity_scores(str(text))  \n",
    "    # Convert to string to avoid errors\n",
    "    if score['compound'] >= 0.05:\n",
    "        return \"Positive\"\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Check if 'cleaned_tweet' exists before applying sentiment analysis\n",
    "if \"cleaned_tweet\" in df.columns:\n",
    "    df[\"Sentiment\"] = df[\"cleaned_tweet\"].apply(get_sentiment_vader)\n",
    "else:\n",
    "    print(\"Error: 'cleaned_tweet' column not found in dataset.\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save the modified dataset\n",
    "df.to_csv(\"Vadersentiment_finaldataset.csv\", index=False)\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa786c14-0bad-4bf3-90c2-2d4af59e4332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|    | Year   |   Popular |   Positive |   Positive, % |   Negative |   Negative, % |   Neutral |   Neutral, % |\n",
      "+====+========+===========+============+===============+============+===============+===========+==============+\n",
      "|  0 | 2015   |        93 |         56 |         60.22 |         21 |         22.58 |        16 |        17.2  |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  1 | 2016   |       294 |        175 |         59.52 |         81 |         27.55 |        38 |        12.93 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  2 | 2017   |       240 |         92 |         38.33 |         87 |         36.25 |        61 |        25.42 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  3 | 2018   |       390 |        215 |         55.13 |        114 |         29.23 |        61 |        15.64 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  4 | 2019   |       407 |        203 |         49.88 |        147 |         36.12 |        57 |        14    |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  5 | 2020   |       378 |        198 |         52.38 |        123 |         32.54 |        57 |        15.08 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  6 | 2021   |       226 |        106 |         46.9  |         84 |         37.17 |        36 |        15.93 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  7 | 2022   |       135 |         77 |         57.04 |         43 |         31.85 |        15 |        11.11 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  8 | 2023   |       861 |        315 |         36.59 |        417 |         48.43 |       129 |        14.98 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  9 | 2024   |       389 |        166 |         42.67 |        170 |         43.7  |        53 |        13.62 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "| 10 | 2025   |       290 |        159 |         54.83 |        100 |         34.48 |        31 |        10.69 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "| 11 | Total  |      3703 |       1762 |         47.58 |       1387 |         37.46 |       554 |        14.96 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"Vadersentiment_finaldataset.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group by 'year' and count total tweets\n",
    "summary = df.groupby(\"year\")[\"Sentiment\"].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Calculate total tweets per year\n",
    "summary[\"Popular\"] = summary.sum(axis=1)\n",
    "\n",
    "# Calculate percentages correctly\n",
    "summary[\"Positive, %\"] = (summary[\"Positive\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Negative, %\"] = (summary[\"Negative\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Neutral, %\"] = (summary[\"Neutral\"] / summary[\"Popular\"]) * 100\n",
    "\n",
    "# Convert to integer values where needed\n",
    "summary = summary.astype({\"Positive\": int, \"Negative\": int, \"Neutral\": int, \"Popular\": int})\n",
    "summary = summary.round(2)  # Round percentages to 2 decimal places\n",
    "\n",
    "# Reset index to move 'year' into columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Add total row\n",
    "total_row = pd.DataFrame({\n",
    "    \"year\": [\"Total\"],\n",
    "    \"Popular\": [summary[\"Popular\"].sum()],\n",
    "    \"Positive\": [summary[\"Positive\"].sum()],\n",
    "    \"Negative\": [summary[\"Negative\"].sum()],\n",
    "    \"Neutral\": [summary[\"Neutral\"].sum()],\n",
    "})\n",
    "\n",
    "# Correct percentage calculations for the total row\n",
    "total_row[\"Positive, %\"] = (total_row[\"Positive\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Negative, %\"] = (total_row[\"Negative\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Neutral, %\"] = (total_row[\"Neutral\"] / total_row[\"Popular\"]) * 100\n",
    "\n",
    "# Round percentages\n",
    "total_row = total_row.round(2)\n",
    "\n",
    "# Ensure columns match before concatenation\n",
    "summary = pd.concat([summary, total_row], ignore_index=True)\n",
    "\n",
    "# Ensure correct column ordering (Year should be the second column)\n",
    "column_order = [\"year\", \"Popular\", \"Positive\", \"Positive, %\", \"Negative\", \"Negative, %\", \"Neutral\", \"Neutral, %\"]\n",
    "summary = summary[column_order]\n",
    "\n",
    "# Rename \"year\" to \"Year\"\n",
    "summary.rename(columns={\"year\": \"Year\"}, inplace=True)\n",
    "\n",
    "# Format table using tabulate for proper alignment\n",
    "formatted_table = tabulate(summary, headers=\"keys\", tablefmt=\"grid\")\n",
    "\n",
    "# Print the formatted table\n",
    "print(formatted_table)\n",
    "\n",
    "# Save to CSV\n",
    "summary.to_csv(\"sentiment_summary.csv\", index=False)\n",
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb49b70-18da-4394-80b4-6c481a95120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b5ed49b-1d90-4293-8e7f-5f3d551c552c",
   "metadata": {},
   "source": [
    "## 2)PMI Sentiment_Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c79a12d-1292-4dd3-9676-e99e05566e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       cleaned_tweet PMI_Sentiment\n",
      "0  procreate coloring book 16 blooming chibi natu...      Positive\n",
      "1  bear bones vehicles would out sell the planed ...      Positive\n",
      "2  it sounds like youre picking up on the intense...      Positive\n",
      "3  blends adventure crypto and wildlife conservat...      Positive\n",
      "4  that alone raises questions about the nature o...      Positive\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "      <th>tokens</th>\n",
       "      <th>PMI_Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>2024</td>\n",
       "      <td>2 in a couple of years the ummayads literally ...</td>\n",
       "      <td>[2, in, a, couple, of, years, the, ummayads, l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>2016</td>\n",
       "      <td>enter to win this competition win a personalis...</td>\n",
       "      <td>[enter, to, win, this, competition, win, a, pe...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>2019</td>\n",
       "      <td>you are done for at least the next 4yrs klaus ...</td>\n",
       "      <td>[you, are, done, for, at, least, the, next, 4y...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>2017</td>\n",
       "      <td>here you go plastic</td>\n",
       "      <td>[here, you, go, plastic]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>2017</td>\n",
       "      <td>plastic</td>\n",
       "      <td>[plastic]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2020</td>\n",
       "      <td>climate change is real climate crisis is a hoax</td>\n",
       "      <td>[climate, change, is, real, climate, crisis, i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>2020</td>\n",
       "      <td>new york state amends landmark climate change ...</td>\n",
       "      <td>[new, york, state, amends, landmark, climate, ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3557</th>\n",
       "      <td>2025</td>\n",
       "      <td>they had no intentions to follow through with ...</td>\n",
       "      <td>[they, had, no, intentions, to, follow, throug...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>2019</td>\n",
       "      <td>emission schedule looks solid excited</td>\n",
       "      <td>[emission, schedule, looks, solid, excited]</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2024</td>\n",
       "      <td>hopefully everyone has woke up to the fact tha...</td>\n",
       "      <td>[hopefully, everyone, has, woke, up, to, the, ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                      cleaned_tweet  \\\n",
       "3311  2024  2 in a couple of years the ummayads literally ...   \n",
       "335   2016  enter to win this competition win a personalis...   \n",
       "1130  2019  you are done for at least the next 4yrs klaus ...   \n",
       "468   2017                                here you go plastic   \n",
       "562   2017                                            plastic   \n",
       "1479  2020    climate change is real climate crisis is a hoax   \n",
       "1508  2020  new york state amends landmark climate change ...   \n",
       "3557  2025  they had no intentions to follow through with ...   \n",
       "1057  2019              emission schedule looks solid excited   \n",
       "2947  2024  hopefully everyone has woke up to the fact tha...   \n",
       "\n",
       "                                                 tokens PMI_Sentiment  \n",
       "3311  [2, in, a, couple, of, years, the, ummayads, l...      Positive  \n",
       "335   [enter, to, win, this, competition, win, a, pe...      Positive  \n",
       "1130  [you, are, done, for, at, least, the, next, 4y...      Positive  \n",
       "468                            [here, you, go, plastic]       Neutral  \n",
       "562                                           [plastic]       Neutral  \n",
       "1479  [climate, change, is, real, climate, crisis, i...       Neutral  \n",
       "1508  [new, york, state, amends, landmark, climate, ...      Positive  \n",
       "3557  [they, had, no, intentions, to, follow, throug...      Positive  \n",
       "1057        [emission, schedule, looks, solid, excited]       Neutral  \n",
       "2947  [hopefully, everyone, has, woke, up, to, the, ...      Positive  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from nrclex import NRCLex\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"C:\\Users\\nivis\\OneDrive\\Desktop\\finaldataset.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Check required column\n",
    "if 'cleaned_tweet' not in df.columns:\n",
    "    raise ValueError(\"Missing 'cleaned_tweet' column\")\n",
    "\n",
    "# Tokenize\n",
    "df['tokens'] = df['cleaned_tweet'].astype(str).apply(lambda x: x.lower().split())\n",
    "\n",
    "# Build vocabulary and frequency\n",
    "all_tokens = [word for tokens in df['tokens'] for word in tokens]\n",
    "word_freq = Counter(all_tokens)\n",
    "total_words = sum(word_freq.values())\n",
    "\n",
    "# Build co-occurrence dictionary (within each tweet)\n",
    "co_occurrence = defaultdict(int)\n",
    "for tokens in df['tokens']:\n",
    "    unique_tokens = set(tokens)\n",
    "    for w1 in unique_tokens:\n",
    "        for w2 in unique_tokens:\n",
    "            if w1 != w2:\n",
    "                co_occurrence[(w1, w2)] += 1\n",
    "\n",
    "# Define domain-specific sentiment words\n",
    "positive_words = [\"good\", \"great\", \"excellent\", \"positive\", \"happy\", \"eco-friendly\",\n",
    "                  \"renewable\", \"clean\", \"green\", \"sustainable\", \"alternative\", \"electric\"]\n",
    "\n",
    "negative_words = [\"bad\", \"terrible\", \"pollution\", \"emission\", \"dirty\", \"waste\",\n",
    "                  \"hazardous\", \"climate crisis\", \"global warming\", \"deforestation\", \n",
    "                  \"fire\", \"fossil\", \"smog\"]\n",
    "\n",
    "# PMI Calculation\n",
    "def calculate_pmi(w1, w2):\n",
    "    co_count = co_occurrence.get((w1, w2), 0) + 1  # Add-1 smoothing\n",
    "    p_joint = co_count / total_words\n",
    "    p_w1 = (word_freq[w1] + 1) / total_words\n",
    "    p_w2 = (word_freq[w2] + 1) / total_words\n",
    "    return math.log(p_joint / (p_w1 * p_w2), 2)\n",
    "\n",
    "# Semantic Orientation (SO) for a word\n",
    "def semantic_orientation(word):\n",
    "    if word not in word_freq:\n",
    "        return 0\n",
    "    pos_score = sum(calculate_pmi(word, p) for p in positive_words if p in word_freq)\n",
    "    neg_score = sum(calculate_pmi(word, n) for n in negative_words if n in word_freq)\n",
    "    return (pos_score - neg_score) / word_freq[word]\n",
    "\n",
    "# Tweet-level sentiment using SO\n",
    "def get_sentiment_SO(tokens):\n",
    "    so_score = sum(semantic_orientation(w) for w in tokens)\n",
    "    if so_score > 0.1:\n",
    "        return \"Positive\"\n",
    "    elif so_score < -0.1:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "df['PMI_Sentiment'] = df['tokens'].apply(get_sentiment_SO)\n",
    "\n",
    "df.to_csv(\"finaldataset_with_PMI_sentiment.csv\", index=False)\n",
    "\n",
    "# Preview\n",
    "print(df[['cleaned_tweet', 'PMI_Sentiment']].head())\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae703167-bda1-4076-a47d-bf170e784e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|    | Year   |   Popular |   Positive |   Positive, % |   Negative |   Negative, % |   Neutral |   Neutral, % |\n",
      "+====+========+===========+============+===============+============+===============+===========+==============+\n",
      "|  0 | 2015   |        93 |         91 |         97.85 |          0 |          0    |         2 |         2.15 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  1 | 2016   |       294 |        285 |         96.94 |          1 |          0.34 |         8 |         2.72 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  2 | 2017   |       240 |        222 |         92.5  |          3 |          1.25 |        15 |         6.25 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  3 | 2018   |       390 |        374 |         95.9  |          0 |          0    |        16 |         4.1  |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  4 | 2019   |       407 |        390 |         95.82 |          5 |          1.23 |        12 |         2.95 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  5 | 2020   |       378 |        371 |         98.15 |          1 |          0.26 |         6 |         1.59 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  6 | 2021   |       226 |        218 |         96.46 |          1 |          0.44 |         7 |         3.1  |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  7 | 2022   |       135 |        132 |         97.78 |          0 |          0    |         3 |         2.22 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  8 | 2023   |       861 |        835 |         96.98 |          5 |          0.58 |        21 |         2.44 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "|  9 | 2024   |       389 |        379 |         97.43 |          3 |          0.77 |         7 |         1.8  |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "| 10 | 2025   |       290 |        285 |         98.28 |          0 |          0    |         5 |         1.72 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n",
      "| 11 | Total  |      3703 |       3582 |         96.73 |         19 |          0.51 |       102 |         2.75 |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"finaldataset_with_PMI_sentiment.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Group by 'year' and count total tweets\n",
    "summary = df.groupby(\"year\")[\"PMI_Sentiment\"].value_counts().unstack().fillna(0)\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary.columns = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Calculate total tweets per year\n",
    "summary[\"Popular\"] = summary.sum(axis=1)\n",
    "\n",
    "# Calculate percentages correctly\n",
    "summary[\"Positive, %\"] = (summary[\"Positive\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Negative, %\"] = (summary[\"Negative\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Neutral, %\"] = (summary[\"Neutral\"] / summary[\"Popular\"]) * 100\n",
    "\n",
    "# Convert to integer values where needed\n",
    "summary = summary.astype({\"Positive\": int, \"Negative\": int, \"Neutral\": int, \"Popular\": int})\n",
    "summary = summary.round(2)  # Round percentages to 2 decimal places\n",
    "\n",
    "# Reset index to move 'year' into columns\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Add total row\n",
    "total_row = pd.DataFrame({\n",
    "    \"year\": [\"Total\"],\n",
    "    \"Popular\": [summary[\"Popular\"].sum()],\n",
    "    \"Positive\": [summary[\"Positive\"].sum()],\n",
    "    \"Negative\": [summary[\"Negative\"].sum()],\n",
    "    \"Neutral\": [summary[\"Neutral\"].sum()],\n",
    "})\n",
    "\n",
    "# Correct percentage calculations for the total row\n",
    "total_row[\"Positive, %\"] = (total_row[\"Positive\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Negative, %\"] = (total_row[\"Negative\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Neutral, %\"] = (total_row[\"Neutral\"] / total_row[\"Popular\"]) * 100\n",
    "\n",
    "# Round percentages\n",
    "total_row = total_row.round(2)\n",
    "\n",
    "# Ensure columns match before concatenation\n",
    "summary = pd.concat([summary, total_row], ignore_index=True)\n",
    "\n",
    "# Ensure correct column ordering (Year should be the second column)\n",
    "column_order = [\"year\", \"Popular\", \"Positive\", \"Positive, %\", \"Negative\", \"Negative, %\", \"Neutral\", \"Neutral, %\"]\n",
    "summary = summary[column_order]\n",
    "\n",
    "# Rename \"year\" to \"Year\"\n",
    "summary.rename(columns={\"year\": \"Year\"}, inplace=True)\n",
    "\n",
    "# Format table using tabulate for proper alignment\n",
    "formatted_table = tabulate(summary, headers=\"keys\", tablefmt=\"grid\")\n",
    "\n",
    "# Print the formatted table\n",
    "print(formatted_table)\n",
    "\n",
    "# Save to CSV\n",
    "summary.to_csv(\"sentiment_summary.csv\", index=False)\n",
    "##pmi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d2712-07a2-4879-a6ad-3a6c8042285c",
   "metadata": {},
   "source": [
    "## 3) Emotional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c934b94c-35dd-46ae-8731-2d3c8ef45112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|    | Year   |   Popular |   Positive |   Positive, % |   Negative |   Negative, % |   Neutral |   Neutral, % | Emotion      |\n",
      "+====+========+===========+============+===============+============+===============+===========+==============+==============+\n",
      "|  0 | 2015   |        93 |         56 |         60.22 |         21 |         22.58 |        16 |        17.2  | None         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  1 | 2016   |       294 |        175 |         59.52 |         81 |         27.55 |        38 |        12.93 | trust        |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  2 | 2017   |       240 |         92 |         38.33 |         87 |         36.25 |        61 |        25.42 | None         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  3 | 2018   |       390 |        215 |         55.13 |        114 |         29.23 |        61 |        15.64 | None         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  4 | 2019   |       407 |        203 |         49.88 |        147 |         36.12 |        57 |        14    | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  5 | 2020   |       378 |        198 |         52.38 |        123 |         32.54 |        57 |        15.08 | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  6 | 2021   |       226 |        106 |         46.9  |         84 |         37.17 |        36 |        15.93 | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  7 | 2022   |       135 |         77 |         57.04 |         43 |         31.85 |        15 |        11.11 | trust        |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  8 | 2023   |       861 |        315 |         36.59 |        417 |         48.43 |       129 |        14.98 | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "|  9 | 2024   |       389 |        166 |         42.67 |        170 |         43.7  |        53 |        13.62 | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "| 10 | 2025   |       290 |        159 |         54.83 |        100 |         34.48 |        31 |        10.69 | anticipation |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n",
      "| 11 | Total  |      3703 |       1762 |         47.58 |       1387 |         37.46 |       554 |        14.96 | fear         |\n",
      "+----+--------+-----------+------------+---------------+------------+---------------+-----------+--------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from nrclex import NRCLex\n",
    "\n",
    "# Loading the  dataset\n",
    "file_path = r\"Vadersentiment_finaldataset.csv\"  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to get dominant emotion for each tweet\n",
    "def get_dominant_emotion(text):\n",
    "    try:\n",
    "        emotion = NRCLex(str(text))\n",
    "        scores = emotion.raw_emotion_scores\n",
    "        \n",
    "        # Keep only the 8 basic emotions\n",
    "        core_emotions = ['fear', 'anger', 'anticipation', 'trust', \n",
    "                         'surprise', 'sadness', 'disgust', 'joy']\n",
    "        filtered = {k: v for k, v in scores.items() if k in core_emotions}\n",
    "        \n",
    "        if filtered:\n",
    "            return max(filtered, key=filtered.get)\n",
    "        else:\n",
    "            return \"None\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"None\"\n",
    "\n",
    "# Apply emotion extraction\n",
    "df['Emotion'] = df['cleaned_tweet'].apply(get_dominant_emotion)\n",
    "\n",
    "# Group by 'year' and summarize sentiment\n",
    "summary = df.groupby(\"year\")[\"Sentiment\"].value_counts().unstack().fillna(0)\n",
    "summary.columns = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "summary[\"Popular\"] = summary.sum(axis=1)\n",
    "\n",
    "# Calculate sentiment percentages\n",
    "summary[\"Positive, %\"] = (summary[\"Positive\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Negative, %\"] = (summary[\"Negative\"] / summary[\"Popular\"]) * 100\n",
    "summary[\"Neutral, %\"] = (summary[\"Neutral\"] / summary[\"Popular\"]) * 100\n",
    "\n",
    "# Convert to int and round %\n",
    "summary = summary.astype({\"Positive\": int, \"Negative\": int, \"Neutral\": int, \"Popular\": int})\n",
    "summary = summary.round(2)\n",
    "summary.reset_index(inplace=True)\n",
    "\n",
    "# Add dominant emotion per year\n",
    "emotion_mode = df.groupby(\"year\")[\"Emotion\"].agg(lambda x: x.mode()[0] if not x.mode().empty else \"None\")\n",
    "summary[\"Emotion\"] = summary[\"year\"].map(emotion_mode)\n",
    "\n",
    "# Add total row\n",
    "total_row = pd.DataFrame({\n",
    "    \"year\": [\"Total\"],\n",
    "    \"Popular\": [summary[\"Popular\"].sum()],\n",
    "    \"Positive\": [summary[\"Positive\"].sum()],\n",
    "    \"Negative\": [summary[\"Negative\"].sum()],\n",
    "    \"Neutral\": [summary[\"Neutral\"].sum()],\n",
    "})\n",
    "total_row[\"Positive, %\"] = (total_row[\"Positive\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Negative, %\"] = (total_row[\"Negative\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Neutral, %\"] = (total_row[\"Neutral\"] / total_row[\"Popular\"]) * 100\n",
    "total_row[\"Emotion\"] = [df[\"Emotion\"].mode()[0]]  # Most common emotion overall\n",
    "total_row = total_row.round(2)\n",
    "\n",
    "# Combine summary and total row\n",
    "summary = pd.concat([summary, total_row], ignore_index=True)\n",
    "\n",
    "# Reorder columns\n",
    "column_order = [\"year\", \"Popular\", \"Positive\", \"Positive, %\", \"Negative\", \"Negative, %\", \"Neutral\", \"Neutral, %\", \"Emotion\"]\n",
    "summary = summary[column_order]\n",
    "\n",
    "# Rename for presentation\n",
    "summary.rename(columns={\"year\": \"Year\"}, inplace=True)\n",
    "\n",
    "# Print table\n",
    "formatted_table = tabulate(summary, headers=\"keys\", tablefmt=\"grid\")\n",
    "print(formatted_table)\n",
    "\n",
    "# Save as CSV\n",
    "summary.to_csv(\"sentiment_emotion_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d28c0a-4405-4abe-8898-f844777dcdb6",
   "metadata": {},
   "source": [
    "## 4)BERTopic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72217ce3-8725-4cb2-a450-13039d833958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic clusters per social media platform and prevailing emotion (Twitter):\n",
      "     Overall         Fear        Trust Anticipation\n",
      "         air          air          air          air\n",
      "   recycling    recycling    recycling    recycling\n",
      "     plastic      plastic      science      science\n",
      "biodiversity      plastic biodiversity biodiversity\n",
      "     plastic biodiversity      plastic      plastic\n",
      "\n",
      "Saved to 'bertopic_emotion_table.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "path = r\"Vadersentiment_finaldataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df['cleaned_tweet'] = df['cleaned_tweet'].astype(str)\n",
    "\n",
    "tweets = df['cleaned_tweet'].tolist()\n",
    "topic_model = BERTopic(language=\"english\", top_n_words=5)\n",
    "topics, probs = topic_model.fit_transform(tweets)\n",
    "df['topic'] = topics\n",
    "\n",
    "environment_keywords = [\n",
    "    \"biodiversity\", \"ecology\", \"air pollution\", \"emission\", \"climate change\",\n",
    "    \"plastic\", \"recycling\", \"global warming\", \"sustainability\", \"greenhouse gases\", \"carbon footprint\"\n",
    "]\n",
    "\n",
    "def topic_contains_environmental_keyword(topic_words, keywords):\n",
    "    topic_text = \" \".join([word for word, _ in topic_words]).lower()\n",
    "    return any(kw in topic_text for kw in keywords)\n",
    "\n",
    "allowed_topic_ids = []\n",
    "for topic_id in df['topic'].unique():\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    if topic_words and topic_contains_environmental_keyword(topic_words, environment_keywords):\n",
    "        allowed_topic_ids.append(topic_id)\n",
    "\n",
    "# Keep only rows with allowed environmental topics\n",
    "df = df[df['topic'].isin(allowed_topic_ids)]\n",
    "\n",
    "target_emotions = [\"fear\", \"trust\", \"anticipation\"]\n",
    "\n",
    "def has_emotion(text, emotion):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return False\n",
    "    try:\n",
    "        emo = NRCLex(text)\n",
    "        return emotion in emo.raw_emotion_scores\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "for emotion in target_emotions:\n",
    "    df[emotion] = df['cleaned_tweet'].apply(lambda x: has_emotion(x, emotion))\n",
    "\n",
    "def get_topic_labels(topic_ids):\n",
    "    labels = []\n",
    "    for topic_id in topic_ids:\n",
    "        words_weights = topic_model.get_topic(topic_id)\n",
    "        if words_weights:\n",
    "            # Pick the single most representative word\n",
    "            top_word = words_weights[0][0]\n",
    "            labels.append(top_word)\n",
    "        else:\n",
    "            labels.append(\"No data\")\n",
    "    return labels\n",
    "\n",
    "emotion_topics = {}\n",
    "\n",
    "for emotion in target_emotions:\n",
    "    filtered = df[df[emotion] == True]\n",
    "    top_topic_ids = filtered['topic'].value_counts().head(5).index.tolist()\n",
    "    labels = get_topic_labels(top_topic_ids)\n",
    "    while len(labels) < 5:\n",
    "        labels.append(\"\")\n",
    "    emotion_topics[emotion.capitalize()] = labels\n",
    "\n",
    "top_overall_ids = df['topic'].value_counts().head(5).index.tolist()\n",
    "overall_labels = get_topic_labels(top_overall_ids)\n",
    "while len(overall_labels) < 5:\n",
    "    overall_labels.append(\"\")\n",
    "emotion_topics['Overall'] = overall_labels\n",
    "\n",
    "final_df = pd.DataFrame(emotion_topics)\n",
    "final_df = final_df[[\"Overall\", \"Fear\", \"Trust\", \"Anticipation\"]]\n",
    "\n",
    "print(\"\\nTopic clusters per social media platform and prevailing emotion (Twitter):\")\n",
    "print(final_df.to_string(index=False))\n",
    "\n",
    "final_df.to_csv(\"bertopic_emotion_table.csv\", index=False)\n",
    "print(\"\\nSaved to 'bertopic_emotion_table.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
